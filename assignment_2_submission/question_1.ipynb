{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Optional, Union, Dict\n",
    "from scipy.special import logsumexp\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Networks and EM\n",
    "\n",
    "In the following questions we will be considering a Bayesian Network for modelling variables related to students' ability to get a job (note this is a toy example aiming to encode a few simple intuitions, the values are randomly made).\n",
    "\n",
    "It has the following variables:\n",
    "- Difficulty (D) of a job-related course in the year the student took the course\n",
    "- Effort (E) that the student put into the course and then finding a job\n",
    "- Aptitude (A) of the student\n",
    "- Confidence (C) of the student in the related subject\n",
    "- Grade (G) of the student in the course\n",
    "- Interview (I) that the student took for the job\n",
    "- TuteAttendance (T) of the student in the course\n",
    "- ForumParticipation (F) of the student in the course\n",
    "- SAT (S) that the student got (Scholarly Aptitute Test, a rough measure of Aptitude)\n",
    "- Job (J) - whether the student got the job or not\n",
    "\n",
    "These variables will be the nodes of our Bayesian Network. We will specify how we believe the variables are affected by each other by the structure of the graph, for example Grade's parents are Difficulty, Effort and Aptitude as the values for those variables directly affect the value of Grade.\n",
    "\n",
    "To represent the Bayesian network, we first define two classes, one for Probability Mass Function objects (`PMF`) that have an array and what each axis means, and the second is a general Bayesian Network node (`BNNode`) that has the nodes' state values, its parents, and its pmf (an object on type `PMF`).\n",
    "\n",
    "The PMF array is always meant to be a conditional probability table, with the probabilities given for the last column given the previous columns. For example `PMF(['Difficulty', 'Effort', 'Aptitude', 'Grade'], (3, 3, 5, 5))` represents $p(G|D,E,A)$, and $p(G=g|D=d,E=e,A=a)$ is at index `[d,e,a,g]` of the array.\n",
    "\n",
    "You should read through the definition of these classes, as you will be working with these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "class PMF:\n",
    "    def __init__(self, dims: List['BNNode'], values: np.array):\n",
    "        self.dims = dims\n",
    "        self.values = values\n",
    "    def __repr__(self):\n",
    "        return \"PMF({}, {})\".format(vars2names(self.dims), self.values.shape)\n",
    "\n",
    "class BNNode:\n",
    "    def __init__(self, name, states: List, parents: [List['BNNode']], pmf):\n",
    "        self.name = name\n",
    "        self.states = states\n",
    "        self.num_states = len(self.states)\n",
    "        self.parents = parents\n",
    "        self.pmf = pmf # {'dim_vars': [names], 'value':pmf} where pmf is (num_states,) or (parents1.num_states,...,num_states)\n",
    "    def check_pmf(self, pmf):\n",
    "        assert type(pmf) == PMF, \"Expect pmf to be of type PMF, got {}\".format(type(pmf))\n",
    "        assert len(pmf.dims)>0, \"Expect at least 1 dim, got {} dims\".format(len(pmf.dims))\n",
    "        assert self == pmf.dims[-1], \"Last dim should be {}, got {}\".format(self.name, pmf.dims[-1].name)\n",
    "        # Make sure the pmf is of the correct shape\n",
    "        expected_shapes = tuple(var.num_states for var in pmf.dims)\n",
    "        assert pmf.values.shape == expected_shapes,  \\\n",
    "            \"Got shape {} for pmf, expected {}\".format(pmf.values.shape, expected_shapes)\n",
    "        assert np.allclose(pmf.values.sum(axis=-1), 1), \"Not all last dims sum to 1: sums: {}\".format(pmf.values.sum(axis=-1))\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Define equality of nodes if they have the same name\"\"\"\n",
    "        if isinstance(other, BNNode):\n",
    "            return self.name == other.name\n",
    "        return False\n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "    def __repr__(self):\n",
    "        return \"BNNode[{}]\".format(self.name)\n",
    "        \n",
    "# Useful for debugging!\n",
    "vars2names = lambda var_lst: [var.name for var in var_lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will actually define a specific Bayesian Network for our problem instance using our BN node. Note this has defined the parents of each node, which defines the structue of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Difficulty', 'Effort', 'Aptitute', 'Confidence', 'Grade', 'Interview', 'TuteAttendence', 'ForumParticipation', 'SAT', 'Job']\n"
     ]
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "D = BNNode(\"Difficulty\",['Easy', 'Med', 'Hard'], [], None)\n",
    "E = BNNode(\"Effort\",['Low', 'Med', 'High'], [], None)\n",
    "A = BNNode(\"Aptitute\",[1,2,3,4,5], [], None)\n",
    "C = BNNode(\"Confidence\",['Low', 'Med', 'High'], [], None)\n",
    "G = BNNode(\"Grade\",['F', 'P', 'Cr', 'D', 'HD'], [D,E,A], None)\n",
    "I = BNNode(\"Interview\",['Bad', 'Ok', 'Godd'], [E,A,C], None)\n",
    "T = BNNode(\"TuteAttendence\",['Low', 'Med', 'High'], [E,C], None)\n",
    "F = BNNode(\"ForumParticipation\",['Low', 'Med', 'High'], [E,A,C], None)\n",
    "S = BNNode(\"SAT\",['Low', 'Med', 'High'], [A], None)\n",
    "J = BNNode(\"Job\",['No', 'Yes'], [G,I], None)\n",
    "\n",
    "nodes = [D, E, A, C, G, I, T, F, S, J]\n",
    "print(vars2names(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BN Part 1: General Properties\n",
    "### Question 1.1 [4 marks]\n",
    "\n",
    "1. Draw the Bayesian Network structure for the network defined above (note the structure is defined by the parents of the node specified when declaring each node).\n",
    "2. Write down the factorisation of the joint distribution given this structure.\n",
    "3. Calculate the number of parameters needed to represent the total joint distribution with this factorisation and without any factorisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1.1.1\n",
    "![title](submission_extras/1.png)  \n",
    "1.1.2\n",
    "![title](submission_extras/2.png)\n",
    "1.1.3\n",
    "If we don't use the factorisation, we will need $\\prod_{v \\in V}{number of p(V=v)}= 3*3*5*3*5*3*3*3*3*2 = 109,350$ parameters.  \n",
    "If we use the the factorisation, we only need $\\sum_{v \\in V}{number of p(V=v)} = 3+3+5+3+5+3+3+3+3+2=33$ parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 [6 marks]\n",
    "Do the following conditional indpendencies hold? Prove your answer.\n",
    "\n",
    "1. $C \\perp \\!\\!\\! \\perp G\\mid I$\n",
    "2. $C \\perp \\!\\!\\! \\perp G\\mid E$\n",
    "3. $C \\perp \\!\\!\\! \\perp G\\mid I,E$\n",
    "4. $C \\perp \\!\\!\\! \\perp G\\mid I,E,J$\n",
    "5. $F \\perp \\!\\!\\! \\perp J\\mid I,G$\n",
    "6. $F \\perp \\!\\!\\! \\perp J\\mid I,E,A$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1.  \n",
    "We can give a counterexample (an unblocked path) for this. Observe that for this path: C-I-A-G. Firstly, C-I-A forms observed HH-node, which is unblocked. Secondly, I-A-G forms unobserved TT-node, which is also unblocked. Hence we find an unblocked path: C-I-A-G. Therefore, $C \\perp \\!\\!\\! \\perp G\\mid I$ does not hold. \n",
    "\n",
    "\n",
    "2.  \n",
    "We will go through all six acyclic paths from C to G to prove that they are all blocked.  \n",
    "(1) We find that C-I-A forms unobserved HH-node, which is blocked. Then path C-I-A-G is blocked.  \n",
    "(2) C-F-A forms unobserved HH-node, which is blocked. Then path C-F-A-G is blocked.  \n",
    "(3) T-E-G forms observed TT-node, which is blocked. Then path C-T-E-G is blocked.  \n",
    "(4) I-J-G forms unobserved HH-node, which is blocked. Then path C-I-J-G is blocked.  \n",
    "(5) F-E-G forms observed TT-node, which is blocked. Then path C-F-E-G is blocked.  \n",
    "(6) I-E-G forms observed TT-node, which is blocked. Then path C-I-E-G is blocked.  \n",
    "Therefore, $C \\perp \\!\\!\\! \\perp G\\mid E$ holds.\n",
    "\n",
    "\n",
    "3.  \n",
    "We can give a counterexample (an unblocked path) for this. Observe that for this path: C-I-A-G. Firstly, C-I-A forms observed HH-node, which is unblocked. Secondly, I-A-G forms unobserved TT-node, which is also unblocked. Hence we find an unblocked path: C-I-A-G. Therefore, $C \\perp \\!\\!\\! \\perp G\\mid I,E$ does not hold. \n",
    "\n",
    "\n",
    "4.  \n",
    "We can give a counterexample (an unblocked path) for this. Observe that for this path: C-I-A-G. Firstly, C-I-A forms observed HH-node, which is unblocked. Secondly, I-A-G forms unobserved TT-node, which is also unblocked. Hence we find an unblocked path: C-I-A-G. Therefore, $C \\perp \\!\\!\\! \\perp G\\mid I,E,J$ does not hold. \n",
    "\n",
    "\n",
    "5.  \n",
    "There are many paths from J to F, and we find that all of them will pass node I and G. Hence we only need to prove all J-G-Xs and J-I-Xs are blocked. We find that all Xs (including D, E, A, C) point to I and G, both I and G  point to J, and only I,G are observed in the graph. We can assert that all J-G-Xs and J-I-Xs are observed HT-nodes and hence all paths from J to F are blocked.\n",
    "Therefore, $F \\perp \\!\\!\\! \\perp J\\mid I,G$ holds.\n",
    "\n",
    "\n",
    "6.  \n",
    "We will go through all six acyclic paths from F to J to prove that they are all blocked.  \n",
    "(1) we find that C-I-J forms observed HT-node, which is blocked. Then path F-C-I-J is blocked.  \n",
    "(2) A-I-J forms observed HT-node, which is blocked. Then path F-A-I-J is blocked.  \n",
    "(3) E-I-J forms observed HT-node, which is blocked. Then path F-E-I-J is blocked.  \n",
    "(4) F-A-G forms observed TT-node, which is blocked. Then path F-A-G-J is blocked.  \n",
    "(5) F-E-G forms observed TT-node, which is blocked. Then path F-E-G-J is blocked.  \n",
    "(6) T-E-G forms observed TT-node, which is blocked. Then path F-C-T-E-G-J is blocked.  \n",
    "Therefore, $F \\perp \\!\\!\\! \\perp J\\mid I,E,A$ holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BN Part 2: Learning the Parameters from Data\n",
    "\n",
    "We now want to learn the parameters of the model given observed samples. However we will only have observed samples for some of the variables, specifically all the variables except `Aptitude`. Thus $A$ is a latent variable, and all other variables are observed. Furthermore we will have some parameters of the model, specifically $p(S\\mid A)$ and $p(F\\mid E,A,C)$. We can think of this as putting in information from other sources: we have found a model (maybe from previous research) for how SAT scores $S$ are affected by Aptitude and how Forum Participation $F$ are affected by Effort, Aptitude and Confidence, so we put this information in directly.\n",
    "\n",
    "However as we have a latent variable, we won't be able to solve directly for all the other unknown parameters using Maximum Likelihood Estimation (MLE). So instead we will need to use Expectation Maximisation (EM) to be able to learn the parameters.\n",
    "\n",
    "We first initialise the (other) pmfs to uniform probability, and load in observed samples. The format for the sample data, `sample_idxes`, is a $n\\times d$ array where $n$ is the number of samples (5000) and $d$ is the number of nodes (10). The value in each column is the index of the observed state for that variable, and the columns are in the same order as `nodes`.\n",
    "\n",
    "As `A` is not observed, the indices for that column are all -1.\n",
    "Note that the observed values are encoded as 0:K-1, where K is the number of states in the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "\n",
    "# Initialise parameters to uniform\n",
    "D.pmf = PMF([D], np.ones((3,))/3)\n",
    "E.pmf = PMF([E], np.ones((3,))/3)\n",
    "A.pmf = PMF([A], np.ones((5,))/5)\n",
    "C.pmf = PMF([C], np.ones((3,))/3)\n",
    "\n",
    "G.pmf = PMF([D,E,A,G], np.ones((3,3,5,5))/(5))\n",
    "I.pmf = PMF([E,A,C,I], np.ones((3,5,3,3))/(3))\n",
    "T.pmf = PMF([E,C,T], np.ones((3,3,3))/(3))\n",
    "F.pmf = PMF([E,A,C,F], np.ones((3,5,3,3))/(3))\n",
    "S.pmf = PMF([A,S], np.ones((5,3))/(3))\n",
    "J.pmf = PMF([G,I,J], np.ones((5,3,2))/(2))\n",
    "\n",
    "# Load and set parameters for specific variables\n",
    "S_pmf = np.load(os.path.join(\"data\",\"question_1\",\"S_pmf.npy\"))\n",
    "S.pmf.values = S_pmf\n",
    "F_pmf = np.load(os.path.join(\"data\",\"question_1\",\"F_pmf.npy\"))\n",
    "F.pmf.values = F_pmf\n",
    "\n",
    "for var in nodes:\n",
    "    var.check_pmf(var.pmf) # Make sure the pmfs are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMF(['Aptitute', 'SAT'], (5, 3))\n",
      "[[0.7  0.25 0.05]\n",
      " [0.6  0.3  0.1 ]\n",
      " [0.4  0.5  0.1 ]\n",
      " [0.2  0.6  0.2 ]\n",
      " [0.05 0.45 0.5 ]]\n"
     ]
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "\n",
    "# examine one of the loaded pmfs P(S|A)\n",
    "# Aptitute x SAT\n",
    "print(S.pmf.__repr__())\n",
    "print(S.pmf.values)\n",
    "\n",
    "\n",
    "# print(G.pmf)\n",
    "# print(G.pmf.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n",
      "[[ 1  1 -1  1  3  1  2  2  2  0]\n",
      " [ 1  0 -1  1  0  0  0  0  0  0]\n",
      " [ 0  2 -1  2  4  1  2  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "\n",
    "# You're given a dataset consisting of records of 5000 students\n",
    "# load this, and examine the first three rows\n",
    "sample_idxes = np.load(os.path.join(\"data\",\"question_1\",\"sample_idxes_data.npy\"))\n",
    "print(sample_idxes.shape)\n",
    "print(sample_idxes[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3 [10 Marks]\n",
    "We can now learn the parameters by maximum likelihood estimation for some of the variables. \n",
    "Let $X$ denote the observed variables, $X=[D,E,C,G,I,T,F,S,J]$, and $Z$ denote the unobserved variables, $Z=[A]$. We can denote assignments to the variables by $\\alpha=\\{x_n,z_n\\}_{n=1}^N$ where $x_n$ are the observed assignments for the $n^\\text{th}$ sample and $z_n$ are the unobserved assignments. So $x_n^{(D,d)}=1$ if in the $n^\\text{th}$ sample the value of $D$ was state $d$, and $x_n^{(D,d)}=0$ otherwise. Likewise the unknown assignments for the latent variables are denoted $z_n^{(A,a)}\\in{0,1}$. \n",
    "\n",
    "Also let us represent the parameters of a variable of the model, which represent the conditional probability for that variable w.r.t. its parents $p(\\text{var} \\mid \\text{pa(var)})$, by $\\theta^{var}$. For example, \n",
    "$$p(G=g \\mid D=d,A=a,C=c)=\\theta^{(G)}_{d,a,c,g}.$$\n",
    "\n",
    "Then we can represent the joint probability in this way:\n",
    "\\begin{align*}\n",
    "p(X,Z|\\alpha,\\theta)\n",
    "    &=\\prod_{n}\\left(p(D|x_n,z_n,\\theta^{(D)}) ... p(G|x_n,z_n,D,A,C,\\theta^{(G)}) ...\\right)\\\\\n",
    "    &=\\prod_{n}\\left(\\left(\\prod_{d\\in D}(\\theta^{(D)}_d)^{x_n^{(D,d)}}\\right) ... \\left(\\prod_{d\\in D}\\prod_{a\\in A}\\prod_{c\\in C}\\prod_{g\\in G}(\\theta^{(G)}_{d,a,c,g})^{x_n^{(D,d)}z_n^{(A,a)}x_n^{(C,c)}x_n^{(G,g)}}\\right) ... \\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note we have the values $x_n$ but do not have the values $z_n$.\n",
    "\n",
    "1. Which variables can we derive the MLE solution for their parameters for (note this would mean representing their parameters in terms of $x_n$ only, without $z_n$). Explain why.\n",
    "\n",
    "2. (M-step for easy variables)\n",
    "Show that the MLE solution for $\\theta^{(D)}$ is given by\n",
    "$$\\theta^{(D)}_d = \\frac{\\sum_n x_n^{(D,d)}}{\\sum_{d'}\\sum_n x_n^{(D,d')}}$$\n",
    "and argue that we can generalise to a general MLE solution in terms of a variable $V$ and its ancestors $U_1,...,U_n$\n",
    "$$\\theta^{(V)}_{{u_1}...{u_n}v} = \\frac{\\sum_n x_n^{(U_1,{u_1})}...x_n^{(U_n,{u_n})}x_n^{(V,v)}}{\\sum_{v'}\\left(\\sum_n x_n^{(U_1,{u_1})}...x_n^{(U_n,{u_n})}x_n^{(V,v')}\\right)}$$\n",
    "\n",
    " (Hint: maximise the log of the joint probability, and use Lagrange Multipliers to enforce that the sum of conditionals should sum to 1).\n",
    "\n",
    "3. Code up the general MLE solution and use it to calculate the parameters for the variables you listed in part 1. Update the pmfs of the nodes made in Q1/2 have these parameters. For the $x_n$'s we suggest you use the assignments array we make below, and fit the rest of the solution using the template we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1.3.1  \n",
    "Answer are D E C T  \n",
    "Since they don't have latent variables as their ancestors and they are not latent variables. \n",
    "\n",
    "1.3.2   \n",
    "Firstly, since $\\theta^{(D)}_d$ is the conditional probability of $\\mathbf{D}$, we have $\\sum_{d}\\theta_{d}^{(D)} = 1$\n",
    "Then, we will represent the log of the joint probability and use Lagrange Multipliers:\n",
    "\n",
    "\\begin{align*}\n",
    "  L &= \\log{p(X,Z|\\alpha,\\theta)} + \\lambda \\left(\\sum_{d}\\theta_{d}^{(D)}-1\\right)\\\\\n",
    "    &=\\sum_{n}\\left(p(D|x_n,z_n,\\theta^{(D)}) ... +p(G|x_n,z_n,D,A,C,\\theta^{(G)}) ...\\right)\\\\\n",
    "    &=\\sum_{n}\\left(\\left(\\sum_{d\\in D}\\log{(\\theta^{(D)}_d)} x_n^{(D,d)}\\right) ... +\\left(\\sum_{d\\in D}\\sum_{a\\in A}\\sum_{c\\in C}\\sum_{g\\in G}\\log{(\\theta^{(G)}_{d,a,c,g})}x_n^{(D,d)}z_n^{(A,a)}x_n^{(C,c)}x_n^{(G,g)}\\right) ... \\right)+\\lambda \\left(\\sum_{d}\\theta_{d}^{(D)}-1\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Then we have two partial derivatives of $L=0$:  \n",
    "$$\\frac{\\partial L}{\\partial \\theta_{d}^{(D)}} = \\frac{\\sum_{n}x_{n}^{(D,d)}}{\\theta_{d}^{(D)}} + \\lambda = 0 \\text{ (1), and d is specific here.}$$   \n",
    "$$\\frac{\\partial L}{\\partial \\lambda} = \\sum_{d}\\theta_{d}^{(D)} - 1 = 0$$\n",
    "Then we have: $$\\sum_{d'}\\sum_{n}x_{n}^{(D,d')}  = - \\sum_{d'}\\theta_{d'}^{(D)}\\lambda \\text{  , } d' \\text{ is universal here.}$$  \n",
    "Also we have $\\sum_{d'}\\theta_{d'}^{(D)} = 1$ from $\\frac{\\partial L}{\\partial\\lambda}$:\n",
    "$$\\sum_{d'}\\sum_{n}x_{n}^{(D,d')}  = - \\lambda \\text{(2)}$$ \n",
    "From (1), (2):\n",
    "$$\\frac{\\sum_{n}x_{n}^{(D,d)}}{\\theta_{d}^{(D)}} = \\sum_{d'}\\sum_{n}x_{n}^{(D,d')}$$\n",
    "\n",
    "Therefore, $$\\theta^{(D)}_d = \\frac{\\sum_n x_n^{(D,d)}}{\\sum_{d'}\\sum_n x_n^{(D,d')}}$$\n",
    "\n",
    "Similiarly, we assume there are a variable $V$ and its ancestors $U_1,...U_n$,  \n",
    "the log of the joint probability and using Lagrange Multipliers:  \n",
    "\n",
    "$$L = \\log{p(X,Z|\\alpha,\\theta)} + \\lambda \\left(\\sum_{v}\\theta^{(V)}_{{u_1}...{u_n}v}-1\\right)$$  \n",
    "Assmue two partial derivatives of $L=0$, same as easy variables:   \n",
    "$$\\frac{\\partial L}{\\partial \\theta^{(V)}_{{u_1}...{u_n}v}} =  \\frac{\\sum_{n}x_n^{(U_1,u_1)}...x_n^{(U_n,u_n)}x_n^{(V,v)}}{\\theta^{(V)}_{{u_1}...{u_n}v}} + \\lambda =0 \\text{ (1), and }v \\text{ is specific here.} $$    \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial\\lambda} = \\sum_{v}\\theta^{(V)}_{{u_1}...{u_n}v}-1=0$$\n",
    "\n",
    "Then, we have: \n",
    "$$\\sum_{v'}\\sum_{n}x_n^{(U_1,u_1)}...x_n^{(U_n,u_n)}x_n^{(V,v')} = -\\lambda \\sum_{v'}\\theta^{(V)}_{{u_1}...{u_n}v'}\\text{ , and }v' \\text{ is universal here.}$$\n",
    "Also we have $\\sum_{v'}\\theta^{(V)}_{{u_1}...{u_n}v'} = 1$ from $\\frac{\\partial L}{\\partial\\lambda}$:  \n",
    "$$-\\lambda = \\sum_{v'}\\left(\\sum_n x_n^{(U_1,{u_1})}...x_n^{(U_n,{u_n})}x_n^{(V,v')}\\right)  \\text{ (2)}$$\n",
    "\n",
    "Therefore, from (1), (2):\n",
    "$$\\theta^{(V)}_{{u_1}...{u_n}v} = \\frac{\\sum_n x_n^{(U_1,{u_1})}...x_n^{(U_n,{u_n})}x_n^{(V,v)}}{\\sum_{v'}\\left(\\sum_n x_n^{(U_1,{u_1})}...x_n^{(U_n,{u_n})}x_n^{(V,v')}\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "\n",
    "# Intialise assignments\n",
    "n, v = sample_idxes.shape\n",
    "assignments = np.zeros((n, v, 5)) # 5 as that is the maximum number of states of any variable\n",
    "for i in range(n):\n",
    "    for j in range(v):\n",
    "        assignments[i,j,sample_idxes[i,j]] = 1\n",
    "assignments[:,nodes.index(A)] = np.nan # No assignments as it (A) is latent latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parameters(curr_var, assignments):\n",
    "    # Calculate the shape of the pmf array for the cur_var\n",
    "    pmf_shape = tuple(len(var.states) for var in curr_var.pmf.dims)\n",
    "    # Initialise a pmf array of the shape calculated above with 0 values\n",
    "    var_pmf = np.zeros(pmf_shape)\n",
    "\n",
    "    # TODO: Use assignments to calculate the values for the pmf array\n",
    "\n",
    "    curr_idx = None\n",
    "    parents_idx = []\n",
    "    curr_parents = curr_var.parents\n",
    "    for idx,single_node in enumerate(nodes):\n",
    "        if curr_var == single_node:\n",
    "            curr_idx = idx\n",
    "        for parent in curr_parents:\n",
    "            if parent == single_node:\n",
    "                parents_idx.append(idx)\n",
    "                \n",
    "#     print(var_pmf.shape)\n",
    "#     print(curr_idx)\n",
    "#     print(parents_idx)\n",
    "    # i.i.d variables\n",
    "    if curr_parents == []:\n",
    "        count_curr = assignments[:,curr_idx,0:curr_var.num_states]\n",
    "        sum_curr = sum(count_curr)\n",
    "        total =sum(sum_curr)\n",
    "        var_pmf = sum_curr/total\n",
    "    else:\n",
    "        # dependent variables\n",
    "        # count\n",
    "        for idx_n, single_student_data in enumerate(assignments):\n",
    "            curr_assignment = single_student_data[curr_idx]\n",
    "            curr_value = np.argwhere(curr_assignment==1)[0]\n",
    "            single_needed = single_student_data[parents_idx]\n",
    "            sample = np.argwhere(single_needed==1)\n",
    "\n",
    "            indexing_parents = sample[:,1]            \n",
    "            indexing_total = np.hstack((indexing_parents,curr_value))\n",
    "\n",
    "            var_pmf[tuple(indexing_total)]+=1\n",
    "\n",
    "#         print(var_pmf)\n",
    "        # normalize\n",
    "        parents_dims = []\n",
    "        for idx in parents_idx:\n",
    "            parents_dims.append(list(range(nodes[idx].num_states)))\n",
    "#         print(parents_dims)\n",
    "        all_products = parents_dims[0]\n",
    "        for i,dims in  enumerate(parents_dims):\n",
    "            if i !=0:\n",
    "                all_products = itertools.product(all_products,dims)\n",
    "        \n",
    "        for indexing in all_products:\n",
    "#             print(var_pmf[indexing])\n",
    "            total = sum(var_pmf[indexing])\n",
    "            var_pmf[tuple(indexing)]/=total\n",
    "            \n",
    "#         print(var_pmf)\n",
    "        \n",
    "    # assign the newly calculated pmf values to the curr_var's pmf array\n",
    "    curr_var.pmf.values = var_pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2058, 0.4964, 0.2978])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "\n",
    "# list the nodes that we can derive the MLE solution for\n",
    "mle_nodes = [D,E,C,T] # TODO: fill this in \n",
    "try:\n",
    "    for node in mle_nodes:\n",
    "        calculate_parameters(node, assignments)    \n",
    "except NotImplementedError:\n",
    "    print(\"Need to implement calculate_parameters\")\n",
    "if len(mle_nodes) == 0:\n",
    "    print(\"Need to add nodes to mle_nodes\")\n",
    "D.pmf.values # Should be roughly [0.2,0.5,0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4 [10 Marks]\n",
    "\n",
    "We now turn to how to properly train our latent variable model using the Expectation Maximization algorithm. Feel free to refer to Section 9.3 and Section 9.4 of the Bishop book for a refresher of this content. \n",
    "\n",
    "The objective of EM is to maximise the expectation of the complete log-likelihood w.r.t. the latent variables:\n",
    "\\begin{align*}\n",
    "E_Z[\\log p(X,Z \\mid \\alpha,\\theta)]=\\sum_Zp(Z\\mid X, \\alpha, \\theta)\\log p(X,Z \\mid \\alpha,\\theta).\n",
    "\\end{align*}\n",
    "\n",
    "In the E-step we should be calculate the expected value of assignments to the latent variables with respect to the data and the current parameters. Thus as $A$ is the only latent parameter, we should be able to calculate the expected value of $z_n^{(A,a)}$, which we can denote by $E\\left[z_n^{(A,a)}\\right]$.\n",
    "\n",
    "Furthermore, as the values of $z_n^{(A,a)}$ are 0 or 1, we have\n",
    "\n",
    "\\begin{align*}\n",
    "E\\left[z_n^{(A,a)}\\right]\n",
    "    &=\\sum_{v\\in\\{0,1\\}} v*p\\left(z_n^{(A,a)}=v\\mid x_n,\\theta\\right)\\\\\n",
    "    &= p\\left(z_n^{(A,a)}=1\\mid x_n,\\theta\\right)\\\\\n",
    "    &= \\frac{p\\left(x_n,z_n^{(A,a)}=1\\mid \\theta\\right)}{p(x_n\\mid \\theta)}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "1. (M-step for hard variables)  Given such expected values, we should be able to use MLE to solve for the parameters of all other variables, and get the exact same form as the general solution given in part 2. of the last question. More specifically given latent variable $V$ with observed ancestors $U_i$ and unobserved ancestors $W_i$, where all the latent variables are independent of each other (i.e. the $W_i$ and $V$ are all independent from each other), then the parameters for $V$ can be given by \n",
    "$$\\theta^{(V)}_{{u_1}...{w_1}...v} = \\frac{\\sum_n x_n^{(U_1,{u_1})}...E\\left[z_n^{(W_1,{w_1})}\\right]...E\\left[z_n^{(V,v)}\\right]}{\\sum_{v'}\\left(\\sum_n x_n^{(U_1,{u_1})}...E\\left[z_n^{(W_1,{w_1})}\\right]...E\\left[z_n^{(V,v')}\\right]\\right)}.$$\n",
    "Show this by solving for the parameters that maximise the EM objective (and again use Lagrange Multipliers, the steps should be very similar).\n",
    "\n",
    "2. (E-step) Note that the equation given in 1. can be implemented using the exact same function we wrote in Q1 (calculate_parameters) where the values used in assignments are the expectations $E\\left[z_n^{(A,a)}\\right]$. To calculate these expectations, implement the sample_ll function, which given a single instances' sample values (i.e. the value of each variable in the network) calculates the log probability of that sample under the parameters. Use our template to do this, which will work in the log domain as otherwise we would be multiplying lots of small probabilities which leads to numerical instability. The supporting code will use this function to calculate the expectations and store them in the assignments array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    " {Type your solution to 1.4.1 and the \"show\" part of 1.4.2 here}\n",
    "\n",
    "Since we have:\n",
    "\\begin{align*}\n",
    "E\\left[z_n^{(A,a)}\\right]\n",
    "    &=\\sum_{v\\in\\{0,1\\}} v*p\\left(z_n^{(A,a)}=v\\mid x_n,\\theta\\right)\\\\\n",
    "    &= p\\left(z_n^{(A,a)}=1\\mid x_n,\\theta\\right)\\\\\n",
    "    &= \\frac{p\\left(x_n,z_n^{(A,a)}=1\\mid \\theta\\right)}{p(x_n\\mid \\theta)}\n",
    "\\end{align*}\n",
    "Then, we will represent the log of the joint probability and use Lagrange Multipliers:\n",
    "\\begin{align*}\n",
    "  L &= E_Z[\\log p(X,Z \\mid \\alpha,\\theta)] + \\lambda \\left(\\sum_{v}\\theta^{(V)}_{{u_1}...{w_1}...v}-1\\right)\\\\\n",
    "    &=E[\\sum_{n}\\left(\\left(\\sum_{d\\in D}\\log{(\\theta^{(D)}_d)} x_n^{(D,d)}\\right) ... +\\sum_{d\\in D}\\sum_{a\\in A}\\sum_{c\\in C}\\sum_{g\\in G}\\log{(\\theta^{(G)}_{d,a,c,g})}x_n^{(D,d)}z_n^{(A,a)}x_n^{(C,c)}x_n^{(G,g)}\\right) ... ] + \\lambda \\left(\\sum_{v}\\theta^{(V)}_{{u_1}...{w_1}...v}-1\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Because the property of expectation $E[ma+ n] = mE[a]+n$, we have: \n",
    "\\begin{align*}\n",
    "L =\\sum_{n}\\left(\\left(\\sum_{d\\in D}\\log{(\\theta^{(D)}_d)} x_n^{(D,d)}\\right) ... +\\left(\\sum_{d\\in D}\\sum_{a\\in A}\\sum_{c\\in C}\\sum_{g\\in G}\\log{(\\theta^{(G)}_{d,a,c,g})}x_n^{(D,d)}E[z_n^{(A,a)}]x_n^{(C,c)}x_n^{(G,g)}\\right) ... \\right) + \\lambda \\left(\\sum_{v}\\theta^{(V)}_{{u_1}...{w_1}...v}-1\\right)\n",
    "\\end{align*}\n",
    "Then, we can assmue two partial derivatives of $L=0$ as 1.3 did:  \n",
    "$$\\frac{\\partial L}{\\partial \\theta^{(V)}_{{u_1}...{w_1}...v}} = \\frac{\\sum_n x_n^{(U_1,{u_1})}...E\\left[z_n^{(W_1,{w_1})}\\right]...E\\left[z_n^{(V,v)}\\right]}{\\theta^{(V)}_{{u_1}...{w_1}...v}} + \\lambda = 0 \\text{ (1), and }𝑣\\text{ is a specific latent variable here.}$$  \n",
    "$$\\frac{\\partial L}{\\partial \\lambda} = \\sum_{v}\\theta^{(V)}_{{u_1}...{w_1}...v}-1 = 0$$\n",
    "We can still get the universal latent variable $v'$ equations by the sum of the previous specific equation:  \n",
    "$$ \\sum_{v'}\\sum_n x_n^{(U_1,{u_1})}...E\\left[z_n^{(W_1,{w_1})}\\right]...E\\left[z_n^{(V,v')}\\right] = - \\sum_{v'}\\theta^{(V)}_{{u_1}...{w_1}...v'} \\lambda$$\n",
    "Also we have $\\sum_{v'}\\theta^{(V)}_{{u_1}...{w_1}...v'} = 1$ from $\\frac{\\partial L}{\\partial\\lambda}$:\n",
    "$$ - \\lambda = \\sum_{v'}\\sum_n x_n^{(U_1,{u_1})}...E\\left[z_n^{(W_1,{w_1})}\\right]...E\\left[z_n^{(V,v')}\\right] \\text{ (2)}$$  \n",
    "Therefore,from (1) and (2):  \n",
    "$$\\theta^{(V)}_{{u_1}...{w_1}...v} = \\frac{\\sum_n x_n^{(U_1,{u_1})}...E\\left[z_n^{(W_1,{w_1})}\\right]...E\\left[z_n^{(V,v)}\\right]}{\\sum_{v'}\\left(\\sum_n x_n^{(U_1,{u_1})}...E\\left[z_n^{(W_1,{w_1})}\\right]...E\\left[z_n^{(V,v')}\\right]\\right)}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ll(nodes, single_sample_idxes):\n",
    "    # Assume that nodes is in the topological order, and single_sample_idxes has the state\n",
    "    # index of each variable (including latent variable) for a one instance.\n",
    "    single_sample_idxes = np.array(single_sample_idxes).reshape(-1)\n",
    "    assert len(nodes) == len(single_sample_idxes)\n",
    "    ll = 0\n",
    "    \n",
    "    # TODO: Sum up the log probs of each \n",
    "    for idx, value in enumerate(single_sample_idxes):\n",
    "        curr_node = nodes[idx]\n",
    "        curr_parents = curr_node.parents\n",
    "        \n",
    "        if len(curr_parents)>0:\n",
    "            curr_parents_idx = [nodes.index(parent) for parent in curr_parents]\n",
    "            curr_parents_idx.append(idx)\n",
    "#             print(curr_node)\n",
    "#             print(curr_parents_idx)\n",
    "#             print(curr_node.pmf.values.shape)\n",
    "#             print(single_sample_idxes)\n",
    "            indexes = single_sample_idxes[curr_parents_idx]\n",
    "#             print(indexes)\n",
    "\n",
    "            \n",
    "            curr_log = np.log(curr_node.pmf.values[tuple(indexes)])\n",
    "        else:\n",
    "            curr_log = np.log(curr_node.pmf.values[value])\n",
    "        ll+=curr_log\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01210654 0.02421308 0.02905569 0.13559322 0.79903148]\n"
     ]
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "try:\n",
    "    # Initialise the assignments to 0\n",
    "    intel_assignments = np.zeros((len(sample_idxes), 5))\n",
    "    for i in range(len(sample_idxes)):\n",
    "        instance_probs = []\n",
    "        for state in range(5):\n",
    "            data = np.array(sample_idxes[i])\n",
    "            data[2] = state # Set Intel's value to state\n",
    "            ll = sample_ll(nodes, data)\n",
    "            instance_probs.append(ll)\n",
    "        instance_probs = np.exp(np.array(instance_probs))\n",
    "        intel_assignments[i] = instance_probs/instance_probs.sum() # normalise ourselves\n",
    "    print(intel_assignments[0]) # Should be approx [0.01,0.02,0.03,0.14,0.80]\n",
    "except NotImplementedError:\n",
    "    print(\"Need to implement sample_ll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the EM algorithm\n",
    "Now we can run our EM algorithm to completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial A marginals:  [0.2 0.2 0.2 0.2 0.2]\n",
      "Iteration 0, A marginals: [0.18114851 0.19602761 0.21529265 0.21455572 0.19297552], Norm Diff 0.02943\n",
      "Iteration 1, A marginals: [0.16991324 0.19261293 0.22531907 0.2245983  0.18755646], Norm Diff 0.01920\n",
      "Iteration 2, A marginals: [0.16142276 0.19012827 0.2343292  0.23291535 0.18120442], Norm Diff 0.01640\n",
      "Iteration 3, A marginals: [0.15401524 0.18823729 0.24332191 0.24034539 0.17408017], Norm Diff 0.01566\n",
      "Iteration 4, A marginals: [0.14727167 0.18668574 0.2522383  0.24700604 0.16679825], Norm Diff 0.01499\n",
      "Iteration 5, A marginals: [0.14111844 0.18535221 0.26084245 0.25287024 0.15981666], Norm Diff 0.01403\n",
      "Iteration 6, A marginals: [0.13553879 0.18418671 0.26895989 0.25793621 0.15337839], Norm Diff 0.01286\n",
      "Iteration 7, A marginals: [0.13050752 0.18316648 0.27650328 0.26224729 0.14757543], Norm Diff 0.01164\n",
      "Iteration 8, A marginals: [0.12598723 0.18227507 0.28344895 0.26587547 0.14241328], Norm Diff 0.01045\n",
      "Iteration 9, A marginals: [0.12193482 0.18149526 0.28981162 0.26890399 0.13785431], Norm Diff 0.00935\n",
      "Iteration 10, A marginals: [0.1183059  0.18080915 0.29562618 0.27141579 0.13384299], Norm Diff 0.00836\n",
      "Iteration 11, A marginals: [0.11505673 0.18020055 0.30093609 0.27348728 0.13031936], Norm Diff 0.00747\n",
      "Iteration 12, A marginals: [0.11214571 0.17965653 0.30578649 0.27518559 0.12722569], Norm Diff 0.00669\n",
      "Iteration 13, A marginals: [0.10953475 0.17916733 0.31022058 0.27656799 0.12450936], Norm Diff 0.00600\n",
      "Iteration 14, A marginals: [0.10719    0.17872552 0.31427808 0.27768256 0.12212385], Norm Diff 0.00539\n",
      "Iteration 15, A marginals: [0.10508187 0.17832528 0.31799477 0.27856935 0.12002873], Norm Diff 0.00486\n",
      "Iteration 16, A marginals: [0.10318456 0.17796196 0.32140256 0.27926174 0.11818918], Norm Diff 0.00438\n",
      "Iteration 17, A marginals: [0.10147552 0.17763186 0.32452975 0.27978763 0.11657524], Norm Diff 0.00396\n",
      "Iteration 18, A marginals: [0.09993485 0.17733213 0.32740126 0.2801706  0.11516116], Norm Diff 0.00359\n",
      "Iteration 19, A marginals: [0.0985449  0.17706069 0.33003896 0.28043082 0.11392463], Norm Diff 0.00325\n",
      "Iteration 20, A marginals: [0.09728989 0.17681616 0.33246198 0.28058575 0.11284622], Norm Diff 0.00295\n",
      "Iteration 21, A marginals: [0.09615563 0.17659779 0.33468702 0.28065069 0.11190887], Norm Diff 0.00268\n",
      "Iteration 22, A marginals: [0.09512932 0.17640529 0.33672868 0.28063922 0.11109749], Norm Diff 0.00243\n",
      "Iteration 23, A marginals: [0.09419935 0.17623877 0.33859983 0.28056338 0.11039866], Norm Diff 0.00221\n",
      "Iteration 24, A marginals: [0.0933552  0.17609857 0.34031194 0.28043389 0.10980041], Norm Diff 0.00201\n",
      "Iteration 25, A marginals: [0.09258733 0.17598511 0.34187537 0.28026018 0.109292  ], Norm Diff 0.00183\n",
      "Iteration 26, A marginals: [0.09188713 0.17589882 0.34329971 0.28005052 0.10886383], Norm Diff 0.00166\n",
      "Iteration 27, A marginals: [0.0912468  0.17583997 0.34459399 0.27981199 0.10850725], Norm Diff 0.00151\n",
      "Iteration 28, A marginals: [0.09065934 0.17580865 0.34576684 0.27955062 0.10821454], Norm Diff 0.00137\n",
      "Iteration 29, A marginals: [0.09011848 0.17580468 0.34682668 0.27927144 0.10797872], Norm Diff 0.00124\n",
      "Iteration 30, A marginals: [0.0896186  0.17582761 0.34778171 0.27897859 0.1077935 ], Norm Diff 0.00113\n",
      "Iteration 31, A marginals: [0.08915469 0.17587671 0.34863998 0.27867542 0.1076532 ], Norm Diff 0.00103\n",
      "Iteration 32, A marginals: [0.08872233 0.17595099 0.34940936 0.27836466 0.10755265], Norm Diff 0.00094\n",
      "Iteration 33, A marginals: [0.08831759 0.17604926 0.3500975  0.2780485  0.10748715], Norm Diff 0.00087\n",
      "Iteration 34, A marginals: [0.08793703 0.1761701  0.35071174 0.27772871 0.10745241], Norm Diff 0.00080\n",
      "Iteration 35, A marginals: [0.08757763 0.17631199 0.35125911 0.2774068  0.10744447], Norm Diff 0.00074\n",
      "Iteration 36, A marginals: [0.08723676 0.17647327 0.3517462  0.27708399 0.10745977], Norm Diff 0.00070\n",
      "Iteration 37, A marginals: [0.08691215 0.17665225 0.35217921 0.27676139 0.10749501], Norm Diff 0.00066\n",
      "Iteration 38, A marginals: [0.08660182 0.17684719 0.35256383 0.27643995 0.10754722], Norm Diff 0.00062\n",
      "Iteration 39, A marginals: [0.08630409 0.17705638 0.35290528 0.27612055 0.10761371], Norm Diff 0.00060\n",
      "Iteration 40, A marginals: [0.08601752 0.17727814 0.3532083  0.27580398 0.10769207], Norm Diff 0.00057\n",
      "Iteration 41, A marginals: [0.08574087 0.17751087 0.35347713 0.27549096 0.10778016], Norm Diff 0.00056\n",
      "Iteration 42, A marginals: [0.08547309 0.17775307 0.35371559 0.27518216 0.10787609], Norm Diff 0.00054\n",
      "Iteration 43, A marginals: [0.08521328 0.17800334 0.35392702 0.27487816 0.1079782 ], Norm Diff 0.00053\n",
      "Iteration 44, A marginals: [0.08496065 0.17826041 0.3541144  0.27457948 0.10808506], Norm Diff 0.00052\n",
      "Iteration 45, A marginals: [0.08471452 0.17852314 0.35428033 0.27428656 0.10819545], Norm Diff 0.00051\n",
      "Iteration 46, A marginals: [0.0844743  0.17879053 0.35442708 0.27399976 0.10830833], Norm Diff 0.00050\n",
      "Iteration 47, A marginals: [0.08423946 0.17906169 0.35455661 0.27371939 0.10842284], Norm Diff 0.00049\n",
      "Iteration 48, A marginals: [0.08400951 0.1793359  0.35467067 0.27344566 0.10853827], Norm Diff 0.00048\n",
      "Iteration 49, A marginals: [0.08378401 0.17961251 0.35477073 0.2731787  0.10865404], Norm Diff 0.00047\n",
      "Iteration 50, A marginals: [0.08356255 0.17989103 0.3548581  0.27291862 0.10876971], Norm Diff 0.00046\n",
      "Iteration 51, A marginals: [0.08334473 0.18017103 0.35493393 0.27266541 0.10888491], Norm Diff 0.00046\n",
      "Iteration 52, A marginals: [0.08313016 0.18045219 0.35499922 0.27241905 0.10899939], Norm Diff 0.00045\n",
      "Iteration 53, A marginals: [0.08291848 0.18073427 0.35505485 0.27217944 0.10911296], Norm Diff 0.00044\n",
      "Iteration 54, A marginals: [0.08270932 0.18101709 0.35510163 0.27194646 0.10922549], Norm Diff 0.00044\n",
      "Iteration 55, A marginals: [0.0825023  0.18130054 0.35514029 0.27171996 0.10933691], Norm Diff 0.00043\n",
      "Iteration 56, A marginals: [0.08229705 0.18158454 0.35517148 0.27149974 0.10944718], Norm Diff 0.00043\n",
      "Iteration 57, A marginals: [0.08209322 0.18186905 0.35519584 0.2712856  0.10955629], Norm Diff 0.00043\n",
      "Iteration 58, A marginals: [0.08189042 0.18215407 0.35521392 0.27107734 0.10966426], Norm Diff 0.00042\n",
      "Iteration 59, A marginals: [0.08168832 0.18243959 0.35522628 0.27087471 0.10977109], Norm Diff 0.00042\n",
      "Iteration 60, A marginals: [0.08148656 0.18272565 0.35523344 0.27067751 0.10987684], Norm Diff 0.00042\n",
      "Iteration 61, A marginals: [0.08128483 0.18301224 0.35523589 0.27048551 0.10998153], Norm Diff 0.00041\n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "try:\n",
    "    print(\"Initial A marginals: \", A.pmf.values)\n",
    "    old_norm_diff = 1\n",
    "    for it in range(100):\n",
    "        # E step\n",
    "        apt_assignments = np.zeros((len(sample_idxes), 5))\n",
    "        for i in range(len(sample_idxes)):\n",
    "            instance_probs = []\n",
    "            for state in range(5):\n",
    "                data = np.array(sample_idxes[i])\n",
    "                data[2] = state\n",
    "                ll = sample_ll(nodes, data)\n",
    "                instance_probs.append(ll)\n",
    "            instance_probs = np.exp(np.array(instance_probs))\n",
    "            apt_assignments[i] = instance_probs/instance_probs.sum()\n",
    "        # Set assignments made in E-step\n",
    "        assignments[:,2,:] = apt_assignments\n",
    "        # M step\n",
    "        old_A_pmfs = np.array(A.pmf.values)\n",
    "        for curr_var in nodes:\n",
    "            if curr_var in [S, F]: # Do not override the parameters we loaded in\n",
    "                continue\n",
    "            ind = [nodes.index(var) for var in curr_var.pmf.dims] # [0, 1, 2, 4]\n",
    "            pmf_shape = tuple(len(var.states) for var in curr_var.pmf.dims) # (3,3,5,5)\n",
    "            var_pmf = np.zeros(pmf_shape)\n",
    "            for values in itertools.product(*(range(len(var.states)) for var in curr_var.pmf.dims)):\n",
    "                var_pmf[values] = assignments[:,ind,values].prod(axis=1).sum()\n",
    "            var_pmf /= var_pmf.sum(axis=-1, keepdims=True)\n",
    "            curr_var.pmf.values = var_pmf\n",
    "        # Check for convergence\n",
    "        norm_diff = np.linalg.norm(old_A_pmfs-A.pmf.values)\n",
    "        if (old_norm_diff-norm_diff)/old_norm_diff < 0.005:\n",
    "            print(\"Converged\")\n",
    "            break\n",
    "        old_norm_diff = norm_diff\n",
    "        print(\"Iteration {}, A marginals: {}, Norm Diff {:.5f}\".format(it, A.pmf.values, norm_diff)) \n",
    "        # Should get to roughly [0.05, 0.2, 0.35, 0.3, 0.1], something like [0.08, 0.18, 0.36, 0.27, 0.11]\n",
    "except NotImplementedError:\n",
    "    print(\"Need to implement sample_ll!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BN Part 3: Inference with the learned model\n",
    "\n",
    "If you have not finished Part 2 and don't have a trained model, load in the following parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "part2complete = False # Change this if you have not completed part 2!!!\n",
    "if not part2complete:\n",
    "    with open(os.path.join(\"data\",\"question_1\",\"saved_pmf_vals.pkl\"), 'rb') as fp:\n",
    "        saved_pmf_vals = pickle.load(fp)\n",
    "\n",
    "    for i, var in enumerate(nodes):\n",
    "        var.pmf.values = saved_pmf_vals[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.5 [10 Marks]\n",
    "1. Calculate the following marginals/conditionals using the `sample_idxes` data: $p(J)$, $p(G)$, $p(I)$, $p(J|G)$, $p(J|C)$, $p(J|E)$.\n",
    "\n",
    "2. Implement a function that marginalises over variables. That is, given variables to keep, it sums over all other variables. For example, given \n",
    "`marginaliseConditionals(nodes, [C])` it will sum over all other variables except for $C$. Thus in $C$ it would leave $p(C)$, in $I$ it would leave $p(I|C)$, in $A$ it would leave $p(A)$ as it doesn't have $C$ as an acestor, and in $J$ it would leave $p(J|C)$. Use this to calculate $p(J|C)$, $p(J|A)$, $p(J|E)$ and $p(J|E,A)$. Check your code by comparing to the calculations done above (they should only have slightly differing values). Furthermore calculate $p(J|A)$, which we couldn't do with the sample_idxes.\n",
    "\n",
    " (Hint: use the `multiply_pmf` function provided).\n",
    "\n",
    "3. Calculate $p(E|J)$ (`marginaliseConditionals` should be useful here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(J)\n",
      "[0.8736 0.1264]\n",
      "p(G)\n",
      "[0.2044 0.1786 0.1968 0.185  0.2352]\n",
      "p(I)\n",
      "[0.4616 0.4314 0.107 ]\n",
      "p(J|G)\n",
      "[[0.95303327 0.04696673]\n",
      " [0.95968645 0.04031355]\n",
      " [0.94817073 0.05182927]\n",
      " [0.88432432 0.11567568]\n",
      " [0.66836735 0.33163265]]\n",
      "p(J|C)\n",
      "[[0.88341232 0.11658768]\n",
      " [0.87316239 0.12683761]\n",
      " [0.86470588 0.13529412]]\n",
      "p(J|E)\n",
      "[[0.92983342 0.07016658]\n",
      " [0.87142857 0.12857143]\n",
      " [0.77242682 0.22757318]]\n"
     ]
    }
   ],
   "source": [
    "# 1.5.1: Calculate the marginals/conditionals from the data and print them out here\n",
    "# calculate p(J)\n",
    "jobs = np.zeros((2))\n",
    "for single in sample_idxes:\n",
    "    if single[-1]==0:\n",
    "        jobs[0]+=1\n",
    "    else:\n",
    "        jobs[1]+=1\n",
    "jobs/=sum(jobs)\n",
    "print(\"p(J)\")\n",
    "print(jobs)\n",
    "\n",
    "#calculate p(G)\n",
    "grades = np.zeros((5))\n",
    "for single in sample_idxes:\n",
    "    curr_idx = single[4]\n",
    "    grades[curr_idx]+=1\n",
    "grades/=sum(grades)\n",
    "print(\"p(G)\")\n",
    "print(grades)\n",
    "\n",
    "#calculate p(I)\n",
    "interviews = np.zeros((3))\n",
    "for single in sample_idxes:\n",
    "    curr_idx = single[5]\n",
    "    interviews[curr_idx]+=1\n",
    "interviews/=sum(interviews)\n",
    "print(\"p(I)\")\n",
    "print(interviews)\n",
    "\n",
    "#calculate p(J|G)\n",
    "jobs_given_grades = np.zeros((5,2))\n",
    "for single in sample_idxes:\n",
    "    curr_idx = single[-1]\n",
    "    grade_idx = single[4]\n",
    "    jobs_given_grades[grade_idx][curr_idx]+=1\n",
    "for i in range(5):\n",
    "    jobs_given_grades[i]/=sum(jobs_given_grades[i])\n",
    "print(\"p(J|G)\")\n",
    "print(jobs_given_grades)\n",
    "\n",
    "#calculate p(J|C)\n",
    "jobs_given_confidence = np.zeros((3,2))\n",
    "for single in sample_idxes:\n",
    "    curr_idx = single[-1]\n",
    "    confidence_idx = single[3]\n",
    "    jobs_given_confidence[confidence_idx][curr_idx]+=1\n",
    "for i in range(3):\n",
    "    jobs_given_confidence[i]/=sum(jobs_given_confidence[i])\n",
    "print(\"p(J|C)\")\n",
    "print(jobs_given_confidence)\n",
    "\n",
    "#calculate p(J|E)\n",
    "jobs_given_effort = np.zeros((3,2))\n",
    "for single in sample_idxes:\n",
    "    curr_idx = single[-1]\n",
    "    effort_idx = single[1]\n",
    "    jobs_given_effort[effort_idx][curr_idx]+=1\n",
    "for i in range(3):\n",
    "    jobs_given_effort[i]/=sum(jobs_given_effort[i])\n",
    "print(\"p(J|E)\")\n",
    "print(jobs_given_effort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "def multiply_pmfs(pmf1,pmf2,var):\n",
    "    # Implements marginalising pmf2 over var using pmf1. pmf1 is not modified, but pmf2 is.\n",
    "    # Assumes the following dimensions\n",
    "    # pmf1.dims : (a1,....,an,var)\n",
    "    # pmf2.dims : (b1,.....,bm,var2) where var (and maybe some of a1,...,an) is contained in b1,...,bm\n",
    "\n",
    "    # Make a copy\n",
    "    pmf1 = PMF(list(pmf1.dims), np.array(pmf1.values))\n",
    "    assert var == pmf1.dims[-1], (var.name, vars2names(pmf1.dims))\n",
    "    assert var in pmf2.dims, (var.name, vars2names(pmf2.dims))\n",
    "    a_is = pmf1.dims[:-1]\n",
    "    b_is = pmf2.dims[:-1]\n",
    "    assert var in b_is, (var.name,vars2names(b_is))\n",
    "    b_is.remove(var)\n",
    "\n",
    "    intersect = set(a_is) & set(b_is)\n",
    "\n",
    "    i = pmf2.dims.index(var)\n",
    "    pmf2.values = np.moveaxis(pmf2.values, i, -2)\n",
    "    pmf2.dims = pmf2.dims[:i] + pmf2.dims[i+1:] # Remove var\n",
    "    # Now pmf2: ({b1,...,bm}\\var,var,var2)\n",
    "    pmf1.values = np.expand_dims(pmf1.values,axis=-2)\n",
    "    # Now pmf2: ({a1,...,bn},1,var)\n",
    "    # print(\"Intersect\", vars2names(intersect))\n",
    "    for var_i in intersect:\n",
    "        i = pmf1.dims.index(var_i)\n",
    "        pmf1.values = np.moveaxis(pmf1.values, i, -3)\n",
    "        pmf1.dims = pmf1.dims[:i] + pmf1.dims[i+1:-1] + [var_i] + pmf1.dims[-1:]\n",
    "        i = pmf2.dims.index(var_i)\n",
    "        pmf2.values = np.moveaxis(pmf2.values, i, -3)\n",
    "        # print(pmf2.dims[:i] , pmf2.dims[i+1:-2] , [var_i] , pmf2.dims[-2:])\n",
    "        pmf2.dims = pmf2.dims[:i] + pmf2.dims[i+1:-1] + [var_i] + pmf2.dims[-1:]\n",
    "    # Now pmf2: ({b1,...,bm}\\{var}\\cup {intersect},{intersect},var,var2)\n",
    "    # Now pmf1: ({11,...,an}\\{intersect},{intersect},1,var2)\n",
    "    # print('1:', pmf1, pmf2)\n",
    "    pmf2_other = set(b_is) - intersect\n",
    "    pmf1_other = set(a_is) - intersect\n",
    "    for _ in range(len(pmf2_other)):\n",
    "        pmf1.values = np.expand_dims(pmf1.values,axis=len(pmf1_other))\n",
    "    pmf2.dims = pmf1.dims[:len(pmf1_other)] + pmf2.dims\n",
    "    # print('2:', pmf1, pmf2)\n",
    "    # Now pmf2: ({b1,...,bm}\\{var}\\cup {intersect},{intersect},var,var2)\n",
    "    # Now pmf1: ({11,...,an}\\{intersect},{1,...,1},{intersect},1,var2)\n",
    "    # print(\"Shapes\", pmf1.values.shape, pmf2.values.shape)\n",
    "    pmf2.values = pmf1.values @ pmf2.values\n",
    "    # Now ({11,...,an}\\{intersect},{b1,...,bm}\\{var}\\cup {intersect},{intersect},1,var2)\n",
    "    pmf2.values = np.squeeze(pmf2.values, axis=-2)\n",
    "    # Now ({11,...,an}\\{intersect},{b1,...,bm}\\{var}\\cup {intersect},{intersect},var2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginaliseConditionals(top_order, rem_vars):\n",
    "    # rem_vars are the vars to not marginalise over\n",
    "    for var in top_order:\n",
    "        # Make a copy\n",
    "        new_pmf = PMF(list(var.pmf.dims), np.array(var.pmf.values))\n",
    "\n",
    "        # TODO: Implement marginalising over all variables in var.pmf.dims apart from any vars in rem_var\n",
    "        # Hint: use multiply_pmfs\n",
    "        # intersections/remaining parents are not empty\n",
    "        \n",
    "#         print(\"ITSSSSSSSS\",var)\n",
    "        if len(new_pmf.dims)==1 and new_pmf.dims[0] in [D,E,A,C]:\n",
    "            var.new_pmf = new_pmf\n",
    "            continue\n",
    "#         if var not in rem_vars:\n",
    "#             rem_vars.append(var)\n",
    "#         print(rem_vars)\n",
    "\n",
    "\n",
    "#         print(len(set(new_pmf.dims).difference(set(rem_vars))))\n",
    "        while len(set(new_pmf.dims).difference(set(rem_vars))) != 0:\n",
    "            idx=0\n",
    "#             print(new_pmf.dims[idx] in rem_vars or var==new_pmf.dims[idx])\n",
    "            while new_pmf.dims[idx] in rem_vars or var==new_pmf.dims[idx]:\n",
    "#                 print(new_pmf.dims[idx] in rem_vars or var==new_pmf.dims[idx])\n",
    "                if idx == len(new_pmf.dims)-1:\n",
    "                    idx = -114514\n",
    "                    break\n",
    "                else:\n",
    "                    idx+=1\n",
    "            if idx == -114514:\n",
    "                break\n",
    "            remove_curr = new_pmf.dims[idx]\n",
    "#             print(var,remove_curr)\n",
    "#             print(new_pmf.dims)\n",
    "#             print()\n",
    "            multiply_pmfs(remove_curr.pmf,new_pmf,remove_curr)\n",
    "        \n",
    "        var.new_pmf = new_pmf\n",
    "#         print(var,new_pmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginals for Job\n",
      "[0.88233116 0.11766884]\n",
      "\n",
      "Marginals for Grade\n",
      "[0.20291146 0.17875817 0.19789516 0.18499422 0.23544098]\n",
      "\n",
      "Marginals for Interview\n",
      "[0.46217175 0.430716   0.10711225]\n",
      "\n",
      "Marginals for Job given Grade\n",
      "[[0.95010592 0.04989408]\n",
      " [0.95986037 0.04013963]\n",
      " [0.94654524 0.05345476]\n",
      " [0.88732419 0.11267581]\n",
      " [0.70715949 0.29284051]]\n",
      "\n",
      "Marginals for Job given Confidence\n",
      "[[0.89808225 0.10191775]\n",
      " [0.88363667 0.11636333]\n",
      " [0.86229587 0.13770413]]\n",
      "\n",
      "Marginals for Job given Effort\n",
      "[[0.93234492 0.06765508]\n",
      " [0.8774997  0.1225003 ]\n",
      " [0.77154083 0.22845917]]\n",
      "\n",
      "Marginals for Job given Confidence\n",
      "[[0.89808225 0.10191775]\n",
      " [0.88363667 0.11636333]\n",
      " [0.86229587 0.13770413]]\n",
      "\n",
      "Marginals for Job given Aptitute\n",
      "[[0.92812449 0.07187551]\n",
      " [0.91948149 0.08051851]\n",
      " [0.88735288 0.11264712]\n",
      " [0.85253003 0.14746997]\n",
      " [0.81464202 0.18535798]]\n",
      "\n",
      "Marginals for Job given Effort\n",
      "[[0.93234492 0.06765508]\n",
      " [0.8774997  0.1225003 ]\n",
      " [0.77154083 0.22845917]]\n",
      "\n",
      "Marginals for Job given Effort, Aptitute\n",
      "Dims:  ['Aptitute', 'Effort', 'Job']\n",
      "[[[0.9389722  0.0610278 ]\n",
      "  [0.92979817 0.07020183]\n",
      "  [0.90532418 0.09467582]]\n",
      "\n",
      " [[0.95046441 0.04953559]\n",
      "  [0.94069482 0.05930518]\n",
      "  [0.76274913 0.23725087]]\n",
      "\n",
      " [[0.94543455 0.05456545]\n",
      "  [0.88317502 0.11682498]\n",
      "  [0.75662671 0.24337329]]\n",
      "\n",
      " [[0.91379264 0.08620736]\n",
      "  [0.81820705 0.18179295]\n",
      "  [0.78478925 0.21521075]]\n",
      "\n",
      " [[0.88003755 0.11996245]\n",
      "  [0.80574967 0.19425033]\n",
      "  [0.70308651 0.29691349]]]\n"
     ]
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "\n",
    "# 1.5.2: Use marginaliseConditionals to calculate the marginals/conditionals from the trained BN and print them out here\n",
    "# Check against prev values calculated from the data (if possible)\n",
    "try:\n",
    "    for var in [J, G, I]:\n",
    "        marginaliseConditionals(nodes, [var])\n",
    "        print(\"Marginals for {}\".format(var.name))\n",
    "        print(var.new_pmf.values)\n",
    "        print()\n",
    "    for var1, var2 in [(J, G), (J, C), (J, E)]:\n",
    "        marginaliseConditionals(nodes, [var2])\n",
    "        print(\"Marginals for {} given {}\".format(var1.name, var2.name))\n",
    "        print(var1.new_pmf.values)\n",
    "        print()\n",
    "    for var1, var2 in [(J, C), (J, A), (J, E)]:\n",
    "        marginaliseConditionals(nodes, [var2])\n",
    "        print(\"Marginals for {} given {}\".format(var1.name, var2.name))\n",
    "        print(var1.new_pmf.values)\n",
    "        print()\n",
    "    marginaliseConditionals(nodes, [E, A])\n",
    "    print(\"Marginals for {} given {}, {}\".format(J.name, E.name, A.name))\n",
    "    print(\"Dims: \", vars2names(J.new_pmf.dims))\n",
    "    print(J.new_pmf.values)\n",
    "except NotImplementedError:\n",
    "    print(\"Need to implement marginaliseConditionals!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginals for Effort\n",
      "[[0.42130521 0.39231851 0.18637627]\n",
      " [0.21755015 0.38973346 0.39271639]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.5.1: Calculate p(E|J) here. Should use marginaliseConditionals\n",
    "# We intend to use Bayesian rules to derive p(E|J) = p(J|E)p(E) / p(J)\n",
    "ans = np.zeros((J.num_states,E.num_states))\n",
    "\n",
    "marginaliseConditionals(nodes,[E])\n",
    "p_J_given_E = J.new_pmf.values\n",
    "p_E = E.new_pmf.values\n",
    "\n",
    "marginaliseConditionals(nodes, [J])\n",
    "p_J = J.new_pmf.values\n",
    "\n",
    "for i in range(J.num_states):\n",
    "    for j in range(E.num_states):\n",
    "        ans[i][j] = p_J_given_E[j,i]*p_E[j]/p_J[i]\n",
    "\n",
    "ans = ans/ np.sum(ans,axis=1,keepdims=True)\n",
    "print(\"Marginals for {}\".format(E.name))\n",
    "print(ans)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
